{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49af6305-9809-41d2-b54d-5cb875351b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code modifies: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.set_num_threads(1) # definitely dont need more than this lol\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    # implement a simple replay buffer to save a dependency\n",
    "\n",
    "    def __init__(self, env, rb_len):\n",
    "        self.rb_len = rb_len\n",
    "        self.mdp = {'obs': torch.zeros(self.rb_len, env.observation_space.n) * np.nan,\n",
    "                    'next_obs': torch.zeros(self.rb_len, env.observation_space.n) * np.nan,\n",
    "                    'actions': torch.zeros(self.rb_len) * np.nan,\n",
    "                    'rewards': torch.zeros(self.rb_len) * np.nan,\n",
    "                    'dones': torch.zeros(self.rb_len) * np.nan}\n",
    "        self.n_updates = 0\n",
    "\n",
    "    def add(self, obs, next_obs, action, reward, done):\n",
    "        idx = self.n_updates % len(self.mdp['obs'])\n",
    "        self.mdp['obs'][idx, :] = copy.copy(obs)\n",
    "        self.mdp['next_obs'][idx, :] = copy.copy(next_obs)\n",
    "        self.mdp['rewards'][idx] = reward\n",
    "        self.mdp['dones'][idx] = done\n",
    "        self.mdp['actions'][idx] = action\n",
    "        self.n_updates += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        b_inds = np.arange((~self.mdp['actions'].isnan()).sum())\n",
    "        np.random.shuffle(b_inds)\n",
    "        idx = b_inds[:batch_size]\n",
    "        return {'obs': self.mdp['obs'][idx], \n",
    "                  'next_obs': self.mdp['next_obs'][idx], \n",
    "                  'rewards': self.mdp['rewards'][idx].unsqueeze(dim=1), \n",
    "                  'dones': self.mdp['dones'][idx].unsqueeze(dim=1), \n",
    "                  'actions': self.mdp['actions'][idx].long().unsqueeze(dim=1)}\n",
    "\n",
    "\n",
    "def flatten_obs(x):\n",
    "    output = torch.zeros(4)\n",
    "    output[x] = 1\n",
    "    return output\n",
    "\n",
    "\n",
    "class QNetwork1(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.n, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, env.action_space.n),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class QNetwork2(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.n, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, env.action_space.n),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def get_networks(env, key):\n",
    "    if key == 1:\n",
    "        return QNetwork1(env), QNetwork1(env)\n",
    "    else:\n",
    "        return QNetwork2(env), QNetwork2(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0bb68c-f5a8-481f-8583-b30ad2176b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    # organize params\n",
    "    n_episodes = 1000\n",
    "    batch_size = 16\n",
    "    buffer_size = 2000\n",
    "    gamma = 1\n",
    "    tau = 1\n",
    "\n",
    "\n",
    "def make_env(episode):\n",
    "    # make switching environment\n",
    "    lake_map = ['SH', 'FG'] if episode < 200 else ['SF', 'HG'] # Start, Frozen, Hole, Goal\n",
    "    return gym.make(\"FrozenLake-v1\", desc=lake_map, is_slippery=False, max_episode_steps=2), int(episode > 200)\n",
    "\n",
    "    \n",
    "def run_trial(args, use_cache=False):\n",
    "    env, label = make_env(0)\n",
    "    q_network, target_network = get_networks(env, args.model)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    rb = ReplayBuffer(env, args.buffer_size)\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    obs = flatten_obs(obs)\n",
    "\n",
    "    if use_cache:\n",
    "        write_path = 'cache/{}_{}_{}_{}_{}_{}.p'.format(int(args.replay_ratio), int(args.weight_reset), int(args.model), int(args.trial), int(args.batch_size), int(args.learning_rate*1000000))\n",
    "        if os.path.exists(write_path):\n",
    "            return 1\n",
    "\n",
    "    i_episode = 0\n",
    "    stats = []\n",
    "    global_step = 0\n",
    "    tot_steps = 0\n",
    "    \n",
    "    while i_episode < args.n_episodes:\n",
    "        global_step += 1\n",
    "        action = int(torch.argmax(q_network(obs).detach()))\n",
    "        \n",
    "        next_obs, reward, termination, truncation, info = env.step(action)\n",
    "        next_obs = flatten_obs(next_obs)\n",
    "        \n",
    "        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "        rb.add(obs, next_obs, action, reward, int(termination or truncation))\n",
    "    \n",
    "        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "        obs = copy.copy(next_obs)\n",
    "    \n",
    "        if termination or truncation:\n",
    "            env, env_label = make_env(i_episode)\n",
    "            # print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, reward), end=\"\")\n",
    "            i_episode += 1\n",
    "            obs, _ = env.reset()\n",
    "            obs = flatten_obs(obs)\n",
    "            if args.weight_reset > 0 and ((i_episode + 1) % args.weight_reset) == 0: # reset based off episode\n",
    "                q_network, target_network = get_networks(env, args.model)\n",
    "                optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)\n",
    "            stats.append([i_episode, env_label, reward] + list(q_network(torch.Tensor([1,0,0,0])).detach().numpy()))\n",
    "    \n",
    "        # ALGO LOGIC: training.\n",
    "        if i_episode > args.batch_size:\n",
    "            for rr in range(args.replay_ratio):\n",
    "                data = rb.sample(args.batch_size)\n",
    "                with torch.no_grad(): \n",
    "                    target_max, _ = target_network(data['next_obs']).max(dim=1)\n",
    "                    td_target = data['rewards'] + args.gamma * target_max.unsqueeze(dim=1) * (1 - data['dones'])\n",
    "                old_value = q_network(data['obs']).gather(1, data['actions'])\n",
    "                loss = F.mse_loss(td_target, old_value)\n",
    "    \n",
    "                # optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tot_steps += 1\n",
    "    \n",
    "            # update target \n",
    "            for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):\n",
    "                target_network_param.data.copy_(args.tau * q_network_param.data + (1.0 - args.tau) * target_network_param.data)\n",
    "\n",
    "    write_df = pd.DataFrame(stats,columns=['episode', 'map', 'reward', 'q_left', 'q_down', 'q_right', 'q_up'])\n",
    "    write_df['replay_ratio'] = args.replay_ratio\n",
    "    write_df['weight_reset'] = args.weight_reset\n",
    "    write_df['model'] = args.model\n",
    "    write_df['trial'] = args.trial\n",
    "    write_df['batch_size'] = args.batch_size\n",
    "    write_df['learning_rate'] = args.learning_rate\n",
    "\n",
    "    if use_cache:\n",
    "        write_df.to_pickle(write_path)\n",
    "        return 1\n",
    "    else:\n",
    "        return write_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f84383-afb0-433f-9448-88bebe51fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "\n",
    "if False:\n",
    "    # Only if you want to run a big grid search. requires lots of cores and Dask. Requires a directory \"cache\" to be made.\n",
    "    from dask.distributed import Client, progress\n",
    "    import dask.bag as db\n",
    "\n",
    "    n_trials = 100\n",
    "    lr_list = [0.01, 0.001, 0.0001, 0.00005, 0.00001]\n",
    "    replay_ratio_list = [1, 4, 16, 64, 256]\n",
    "    weight_reset_list = [0, 10, 50, 100, 500]\n",
    "    model_list = [1, 2]\n",
    "\n",
    "    param_list = []\n",
    "    for t in range(n_trials):\n",
    "        for lr in lr_list:\n",
    "            for w in weight_reset_list:\n",
    "                for r in replay_ratio_list:\n",
    "                    for m in model_list:\n",
    "                        args.learning_rate = lr\n",
    "                        args.weight_reset = w\n",
    "                        args.replay_ratio = r\n",
    "                        args.model = m\n",
    "                        args.trial = t\n",
    "                        param_list.append(copy.copy(args))\n",
    "\n",
    "    client = Client()\n",
    "    b = db.from_sequence(param_list)\n",
    "    output = b.map(lambda x: run_trial(x, use_cache=True)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a1833a-8777-42c1-a38e-5137966fb90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.learning_rate = 0.01\n",
    "args.weight_reset = 10\n",
    "args.replay_ratio = 4\n",
    "args.model = 1\n",
    "args.trial = 0\n",
    "\n",
    "a = run_trial(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1db38-d1c2-4f97-8c2f-e9c7e86bb8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9e2c5-b249-42bb-9f9e-bb43cf84a13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
